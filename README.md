# Semantic Video Search ğŸ¬ğŸ”

**Find exact timestamps in videos using natural language queries.** Type "a person wearing a red hat" and get the precise frame where it happensâ€”powered by OpenAI's CLIP and optimized with vectorized NumPy/PyTorch operations.

## âœ¨ Features

- **Semantic Search**: Find video moments using natural language descriptions
- **Fast Vectorized Operations**: Leverages NumPy and PyTorch for sub-second similarity computation
- **Exact Timestamps**: Returns precise frame locations with similarity scores
- **Production Ready**: Error handling, progress bars, and formatted output

## ğŸ¯ Use Cases

- **Content Moderation**: Find specific scenes in user-generated content
- **Video Analytics**: Search security footage for specific events
- **Content Creation**: Locate scenes for editing and compilation
- **Compliance**: Find regulatory violations in recorded meetings
- **Media Management**: Intelligent video cataloging without metadata

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/theamitmehra/semantic-video-search.git
cd semantic-video-search

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install -r requirements.txt
```

### Basic Usage

```python
from semantic_video_search import SemanticVideoSearch

# Initialize search engine (GPU-enabled)
search_engine = SemanticVideoSearch(device="cuda")

# Extract frames from video
frames, timestamps = search_engine.extract_frames(
    "path/to/video.mp4", 
    fps_sample=2  # Sample every 2 frames
)
search_engine.timestamps = timestamps

# Compute embeddings (vectorized batch processing)
search_engine.compute_frame_embeddings(frames, batch_size=32)

# Search for moments
results = search_engine.search(
    "person wearing a red hat",
    top_k=5,
    threshold=0.25
)

# Display results
search_engine.display_results(results, "person wearing a red hat")
```

### Expected Output

```
======================================================================
Query: 'person wearing a red hat'
Found 5 results

1. [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 0.942 @ 00:00:12.50
2. [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0.873 @ 00:00:28.30
3. [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0.792 @ 00:00:45.80
4. [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0.721 @ 00:01:02.10
5. [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0.681 @ 00:01:11.40
======================================================================
```

## ğŸ“Š How It Works

### 1. **Frame Extraction** ğŸ¥
Samples frames from video at configurable rate (e.g., every 2 frames for 30fps video = 0.5s intervals)

### 2. **CLIP Embeddings** ğŸ§ 
Converts frames and text into 512-dimensional vectors using OpenAI's CLIP model:
- Image encoder: Vision Transformer (ViT-B/32)
- Text encoder: Transformer-based language model
- Both mapped to shared embedding space

### 3. **Vectorized Similarity** âš¡
Computes cosine similarity using optimized matrix operations:
```
similarities = frame_embeddings @ query_embedding.T
# Shape: (num_frames,) computed in O(n) with GPU acceleration
```

### 4. **Ranking & Filtering** ğŸ†
Returns top-k results with similarity scores above threshold

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Video Input (MP4/MOV/WebM)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Frame Extract â”‚  (OpenCV, configurable FPS)
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  CLIP Image Encoder â”‚  (ViT-B/32, GPU batch)
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Normalized Embeddings     â”‚  (512-dim vectors)
        â”‚ (tensor shape: Nx512)     â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Text Query        â”‚
        â”‚ CLIP Encoder      â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Vectorized @ Operation      â”‚  (PyTorch matmul)
        â”‚ Cosine Similarity Scores    â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Top-K Filtering &   â”‚
        â”‚ Threshold Filtering â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Results with Timestamps â”‚
        â”‚ & Similarity Scores     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ’» Technology Stack

- **CLIP Model**: `openai/CLIP` (ViT-B/32)
- **Video Processing**: OpenCV (`cv2`)
- **Numerical Computing**: NumPy, PyTorch
